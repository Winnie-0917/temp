"""
æ¡Œçƒå¤±èª¤åˆ†ææ¨¡çµ„ - ä½¿ç”¨ Gemini AI
åˆ†æå¤±åˆ†åŸå› ä¸¦æä¾›æ”¹é€²å»ºè­°
"""
import os
import json
import base64
import cv2
import numpy as np
from typing import Dict, List, Tuple, Optional
import google.generativeai as genai
from skeleton import PoseExtractor
from dotenv import dotenv_values

class FailureAnalyzer:
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–å¤±èª¤åˆ†æå™¨
        
        Args:
            api_key: Gemini API é‡‘é‘°ï¼ˆè‹¥ç„¡å‰‡å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼‰
        """
        # ç›´æ¥å¾ .env æª”æ¡ˆè®€å–
        if not api_key:
            env_config = dotenv_values('.env')
            api_key = env_config.get('GEMINI_API_KEY') or os.getenv('GEMINI_API_KEY')
        
        self.api_key = api_key
        if self.api_key:
            genai.configure(api_key=self.api_key)
            # ä½¿ç”¨ Gemini 2.5 Pro æ¨¡å‹
            self.model = genai.GenerativeModel('gemini-2.5-pro')
        else:
            self.model = None
            print("âš ï¸  æœªè¨­å®š GEMINI_API_KEYï¼Œå°‡ä½¿ç”¨åŸºç¤åˆ†ææ¨¡å¼")
        
        self.pose_extractor = PoseExtractor()
    
    def extract_key_frames(self, video_path: str, num_frames: int = 5) -> List[np.ndarray]:
        """
        å¾å½±ç‰‡ä¸­æŠ½å–é—œéµå¹€
        
        Args:
            video_path: å½±ç‰‡è·¯å¾‘
            num_frames: è¦æŠ½å–çš„å¹€æ•¸
            
        Returns:
            é—œéµå¹€åˆ—è¡¨
        """
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # å‡å‹»åˆ†ä½ˆé¸å–å¹€
        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
        
        frames = []
        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
        
        cap.release()
        return frames
    
    def analyze_pose_sequence(self, frames: List[np.ndarray]) -> Dict:
        """
        åˆ†æå§¿æ…‹åºåˆ—
        
        Args:
            frames: å½±åƒå¹€åˆ—è¡¨
            
        Returns:
            çµæ§‹åŒ–çš„å§¿æ…‹åˆ†ææ•¸æ“š
        """
        pose_data = []
        
        for i, frame in enumerate(frames):
            # ä½¿ç”¨ MediaPipe æå–å§¿æ…‹
            results = self.pose_extractor.extract_pose(frame)
            
            if results and results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                
                # è¨ˆç®—é—œéµé»ä½ç½®
                right_wrist = landmarks[16]  # å³æ‰‹è…•
                right_elbow = landmarks[14]  # å³æ‰‹è‚˜
                right_shoulder = landmarks[12]  # å³è‚©
                
                # è¨ˆç®—æ‹é¢è§’åº¦ï¼ˆç°¡åŒ–ç‰ˆï¼‰
                wrist_pos = np.array([right_wrist.x, right_wrist.y])
                elbow_pos = np.array([right_elbow.x, right_elbow.y])
                arm_vector = wrist_pos - elbow_pos
                racket_angle = np.degrees(np.arctan2(arm_vector[1], arm_vector[0]))
                
                # è¨ˆç®—é‡å¿ƒä½ç½®
                left_hip = landmarks[23]
                right_hip = landmarks[24]
                center_of_mass = {
                    'x': (left_hip.x + right_hip.x) / 2,
                    'y': (left_hip.y + right_hip.y) / 2,
                    'z': (left_hip.z + right_hip.z) / 2
                }
                
                pose_data.append({
                    'frame_index': i,
                    'racket_angle': float(racket_angle),
                    'wrist_height': float(right_wrist.y),
                    'elbow_height': float(right_elbow.y),
                    'shoulder_height': float(right_shoulder.y),
                    'center_of_mass': center_of_mass,
                    'confidence': float(np.mean([lm.visibility for lm in landmarks]))
                })
        
        return {
            'total_frames': len(frames),
            'analyzed_frames': len(pose_data),
            'pose_sequence': pose_data,
            'avg_racket_angle': float(np.mean([p['racket_angle'] for p in pose_data])) if pose_data else 0,
            'racket_angle_variance': float(np.var([p['racket_angle'] for p in pose_data])) if pose_data else 0
        }
    
    def estimate_ball_trajectory(self, frames: List[np.ndarray]) -> Dict:
        """
        ä¼°è¨ˆçƒçš„è»Œè·¡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        
        Args:
            frames: å½±åƒå¹€åˆ—è¡¨
            
        Returns:
            çƒè»Œè·¡åˆ†ææ•¸æ“š
        """
        # é€™è£¡å¯ä»¥ç”¨ YOLO ç­‰ç‰©ä»¶åµæ¸¬ï¼Œç›®å‰ç”¨ç°¡åŒ–ç‰ˆ
        # å¯¦éš›æ‡‰ç”¨å»ºè­°ä½¿ç”¨å°ˆé–€çš„çƒè¿½è¹¤ç®—æ³•
        
        return {
            'trajectory_detected': False,
            'ball_speed': 'medium',
            'spin_type': 'unknown',
            'landing_position': 'unknown',
            'note': 'éœ€è¦æ›´ç²¾ç¢ºçš„çƒè¿½è¹¤æ¨¡å‹ä¾†ç²å¾—è©³ç´°è»Œè·¡'
        }
    
    def generate_structured_analysis(self, video_path: str) -> Dict:
        """
        ç”Ÿæˆçµæ§‹åŒ–åˆ†ææ•¸æ“š
        
        Args:
            video_path: å½±ç‰‡è·¯å¾‘
            
        Returns:
            å®Œæ•´çš„çµæ§‹åŒ–åˆ†æ
        """
        # 1. æŠ½å–é—œéµå¹€
        frames = self.extract_key_frames(video_path, num_frames=5)
        
        # 2. åˆ†æå§¿æ…‹
        pose_analysis = self.analyze_pose_sequence(frames)
        
        # 3. åˆ†æçƒè»Œè·¡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        trajectory = self.estimate_ball_trajectory(frames)
        
        # 4. ç²å–å½±ç‰‡è³‡è¨Š
        cap = cv2.VideoCapture(video_path)
        duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS)
        cap.release()
        
        return {
            'video_info': {
                'duration_seconds': float(duration),
                'analyzed_frames': len(frames)
            },
            'pose_analysis': pose_analysis,
            'ball_trajectory': trajectory,
            'technical_indicators': {
                'stance': self._evaluate_stance(pose_analysis),
                'racket_control': self._evaluate_racket_control(pose_analysis),
                'body_balance': self._evaluate_balance(pose_analysis)
            }
        }
    
    def _evaluate_stance(self, pose_analysis: Dict) -> str:
        """è©•ä¼°ç«™ä½"""
        if not pose_analysis['pose_sequence']:
            return 'unknown'
        
        # ç°¡åŒ–è©•ä¼°é‚è¼¯
        avg_angle = pose_analysis['avg_racket_angle']
        if abs(avg_angle) > 60:
            return 'too_tilted'
        elif abs(avg_angle) < 20:
            return 'too_flat'
        return 'normal'
    
    def _evaluate_racket_control(self, pose_analysis: Dict) -> str:
        """è©•ä¼°æ‹é¢æ§åˆ¶"""
        variance = pose_analysis['racket_angle_variance']
        if variance > 500:
            return 'unstable'
        elif variance > 200:
            return 'moderate'
        return 'stable'
    
    def _evaluate_balance(self, pose_analysis: Dict) -> str:
        """è©•ä¼°èº«é«”å¹³è¡¡"""
        if not pose_analysis['pose_sequence']:
            return 'unknown'
        
        # æª¢æŸ¥é‡å¿ƒè®ŠåŒ–
        com_y_values = [p['center_of_mass']['y'] for p in pose_analysis['pose_sequence']]
        variance = np.var(com_y_values)
        
        if variance > 0.01:
            return 'unstable'
        return 'stable'
    
    def analyze_with_gemini(self, structured_data: Dict, video_path: Optional[str] = None) -> Dict:
        """
        ä½¿ç”¨ Gemini AI é€²è¡Œæ·±åº¦åˆ†æ
        
        Args:
            structured_data: çµæ§‹åŒ–åˆ†ææ•¸æ“š
            video_path: å½±ç‰‡è·¯å¾‘ï¼ˆå¯é¸ï¼Œç”¨æ–¼ç›´æ¥åˆ†æï¼‰
            
        Returns:
            Gemini åˆ†æçµæœ
        """
        if not self.model:
            return {
                'error': 'Gemini API æœªé…ç½®',
                'fallback_analysis': self._basic_analysis(structured_data)
            }
        
        try:
            # æ§‹å»ºæç¤ºè©
            prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ¡Œçƒæ•™ç·´ã€‚è«‹æ ¹æ“šä»¥ä¸‹æŠ€è¡“æ•¸æ“šï¼Œåˆ†æé¸æ‰‹åœ¨é€™æ®µå½±ç‰‡ä¸­çš„å¤±èª¤åŸå› ã€‚

ğŸ“Š æŠ€è¡“æ•¸æ“šï¼š
{json.dumps(structured_data, indent=2, ensure_ascii=False)}

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºåˆ†æçµæœï¼š
{{
  "failure_reason": "å¤±åˆ†çš„ä¸»è¦åŸå› ï¼ˆ50å­—ä»¥å…§ï¼‰",
  "category": "æŠ€è¡“é¡åˆ¥ï¼ˆç«™ä½éŒ¯èª¤/æ—‹è½‰åˆ¤æ–·éŒ¯èª¤/æ‹é¢è§’åº¦/æ™‚é–“å·®/é‡å¿ƒä¸ç©©/å…¶ä»–ï¼‰",
  "detailed_analysis": {{
    "stance": "ç«™ä½åˆ†æ",
    "racket_angle": "æ‹é¢è§’åº¦åˆ†æ",
    "body_balance": "èº«é«”å¹³è¡¡åˆ†æ",
    "timing": "æ“Šçƒæ™‚æ©Ÿåˆ†æ"
  }},
  "improvement_suggestions": [
    "å…·é«”æ”¹é€²å»ºè­°1",
    "å…·é«”æ”¹é€²å»ºè­°2",
    "å…·é«”æ”¹é€²å»ºè­°3"
  ],
  "summary": "ä¸€å¥è©±ç¸½çµï¼ˆ30å­—ä»¥å…§ï¼‰",
  "severity": "å•é¡Œåš´é‡ç¨‹åº¦ï¼ˆminor/moderate/severeï¼‰"
}}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œå°ˆæ³¨æ–¼æŠ€è¡“åˆ†æå’Œå¯¦ç”¨å»ºè­°ã€‚
"""
            
            # å‘¼å« Gemini API
            if video_path and os.path.exists(video_path):
                # å½±ç‰‡ç›´æ¥åˆ†ææ¨¡å¼
                with open(video_path, 'rb') as f:
                    video_data = f.read()
                
                video_base64 = base64.b64encode(video_data).decode('utf-8')
                
                response = self.model.generate_content([
                    {"text": prompt},
                    {"inline_data": {"mime_type": "video/mp4", "data": video_base64}}
                ])
            else:
                # ç´”æ–‡å­—åˆ†ææ¨¡å¼
                response = self.model.generate_content(prompt)
            
            # è§£æå›æ‡‰
            result_text = response.text
            print(f"ğŸ“ Gemini åŸå§‹å›æ‡‰é•·åº¦: {len(result_text)}")
            print(f"ğŸ“ Gemini åŸå§‹å›æ‡‰å‰ 500 å­—å…ƒ:\n{result_text[:500]}")
            
            # å˜—è©¦æå– JSON
            try:
                # ç§»é™¤å¯èƒ½çš„ markdown æ¨™è¨˜
                if '```json' in result_text:
                    result_text = result_text.split('```json')[1].split('```')[0]
                elif '```' in result_text:
                    result_text = result_text.split('```')[1].split('```')[0]
                
                analysis_result = json.loads(result_text.strip())
                print(f"âœ… JSON è§£ææˆåŠŸ")
                print(f"ğŸ“Š è§£æçµæœ: {json.dumps(analysis_result, ensure_ascii=False, indent=2)}")
                analysis_result['source'] = 'gemini'
                return analysis_result
            
            except json.JSONDecodeError as je:
                # å¦‚æœç„¡æ³•è§£æ JSONï¼Œè¿”å›åŸå§‹æ–‡å­—
                print(f"âŒ JSON è§£æå¤±æ•—: {str(je)}")
                print(f"ğŸ” å˜—è©¦è§£æçš„æ–‡å­—:\n{result_text}")
                return {
                    'source': 'gemini',
                    'raw_response': result_text,
                    'parsed': False
                }
        
        except Exception as e:
            print(f"âŒ Gemini åˆ†æå¤±æ•—: {str(e)}")
            return {
                'error': str(e),
                'fallback_analysis': self._basic_analysis(structured_data)
            }
    
    def _basic_analysis(self, structured_data: Dict) -> Dict:
        """åŸºç¤åˆ†æï¼ˆç•¶ Gemini ä¸å¯ç”¨æ™‚ï¼‰"""
        tech_indicators = structured_data.get('technical_indicators', {})
        pose_analysis = structured_data.get('pose_analysis', {})
        
        issues = []
        suggestions = []
        
        # æ ¹æ“šæŠ€è¡“æŒ‡æ¨™çµ¦å‡ºå»ºè­°
        if tech_indicators.get('stance') == 'too_tilted':
            issues.append('æ‹é¢éæ–¼å‚¾æ–œ')
            suggestions.append('èª¿æ•´æ‹é¢è§’åº¦ï¼Œä¿æŒé©ä¸­çš„å‚¾æ–œåº¦')
        
        if tech_indicators.get('racket_control') == 'unstable':
            issues.append('æ‹é¢æ§åˆ¶ä¸ç©©å®š')
            suggestions.append('åŠ å¼·æ‰‹è…•ç©©å®šæ€§è¨“ç·´ï¼Œä¿æŒä¸€è‡´çš„æ“Šçƒå‹•ä½œ')
        
        if tech_indicators.get('body_balance') == 'unstable':
            issues.append('é‡å¿ƒä¸ç©©')
            suggestions.append('æ³¨æ„ä¿æŒä¸‹ç›¤ç©©å®šï¼Œæ“Šçƒæ™‚é‡å¿ƒä¸‹å£“')
        
        if not issues:
            issues.append('ç„¡æ˜é¡¯æŠ€è¡“å•é¡Œ')
            suggestions.append('ç¹¼çºŒä¿æŒè‰¯å¥½çš„æŠ€è¡“å‹•ä½œ')
        
        return {
            'source': 'basic',
            'failure_reason': 'ã€'.join(issues),
            'category': 'æŠ€è¡“å•é¡Œ',
            'improvement_suggestions': suggestions,
            'summary': f"ä¸»è¦å•é¡Œï¼š{issues[0]}" if issues else "å‹•ä½œè‰¯å¥½",
            'severity': 'moderate' if len(issues) > 1 else 'minor'
        }
    
    def analyze_failure(self, video_path: str, use_gemini: bool = True) -> Dict:
        """
        å®Œæ•´çš„å¤±èª¤åˆ†ææµç¨‹
        
        Args:
            video_path: å½±ç‰‡è·¯å¾‘
            use_gemini: æ˜¯å¦ä½¿ç”¨ Gemini åˆ†æ
            
        Returns:
            å®Œæ•´åˆ†æçµæœ
        """
        print(f"ğŸ¬ é–‹å§‹åˆ†æå½±ç‰‡: {video_path}")
        
        # 1. ç”Ÿæˆçµæ§‹åŒ–æ•¸æ“š
        print("ğŸ“Š ç”Ÿæˆçµæ§‹åŒ–åˆ†ææ•¸æ“š...")
        structured_data = self.generate_structured_analysis(video_path)
        
        # 2. ä½¿ç”¨ Gemini åˆ†æï¼ˆæˆ–åŸºç¤åˆ†æï¼‰
        if use_gemini and self.model:
            print("ğŸ¤– ä½¿ç”¨ Gemini AI é€²è¡Œæ·±åº¦åˆ†æ...")
            ai_analysis = self.analyze_with_gemini(structured_data, video_path)
        else:
            print("ğŸ“ ä½¿ç”¨åŸºç¤åˆ†ææ¨¡å¼...")
            ai_analysis = self._basic_analysis(structured_data)
        
        # 3. åˆä½µçµæœ
        return {
            'structured_data': structured_data,
            'ai_analysis': ai_analysis,
            'timestamp': self._get_timestamp()
        }
    
    def _get_timestamp(self) -> str:
        """ç²å–æ™‚é–“æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()


# æ¸¬è©¦ä»£ç¢¼
if __name__ == '__main__':
    # æ¸¬è©¦åˆ†æå™¨
    analyzer = FailureAnalyzer()
    
    # æ¸¬è©¦å½±ç‰‡è·¯å¾‘
    test_video = 'uploads/test_video.mp4'
    
    if os.path.exists(test_video):
        result = analyzer.analyze_failure(test_video)
        print("\n" + "="*50)
        print("åˆ†æçµæœï¼š")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print(f"âš ï¸  æ¸¬è©¦å½±ç‰‡ä¸å­˜åœ¨: {test_video}")
