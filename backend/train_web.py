"""
ç¶²é ç‰ˆè¨“ç·´è…³æœ¬ - ç°¡åŒ–ç‰ˆæœ¬ï¼Œæ”¯æ´é€²åº¦å›å ±
"""

import numpy as np
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.optimizers import Adam
import joblib
from datetime import datetime
from skeleton import PoseExtractor


class TrainingProgressCallback(Callback):
    """è‡ªå®šç¾©å›èª¿å‡½æ•¸ï¼Œç”¨æ–¼æ›´æ–°è¨“ç·´é€²åº¦"""
    
    def __init__(self, task_id, training_tasks, total_epochs):
        super().__init__()
        self.task_id = task_id
        self.training_tasks = training_tasks
        self.total_epochs = total_epochs
    
    def on_epoch_end(self, epoch, logs=None):
        """æ¯å€‹ epoch çµæŸå¾Œæ›´æ–°ç‹€æ…‹"""
        logs = logs or {}
        
        self.training_tasks[self.task_id].update({
            'current_epoch': epoch + 1,
            'accuracy': float(logs.get('accuracy', 0)),
            'val_accuracy': float(logs.get('val_accuracy', 0)),
            'loss': float(logs.get('loss', 0)),
            'val_loss': float(logs.get('val_loss', 0)),
            'message': f'è¨“ç·´ä¸­... Epoch {epoch + 1}/{self.total_epochs}'
        })
        
        log_msg = f"Epoch {epoch + 1}/{self.total_epochs} - acc: {logs.get('accuracy', 0):.4f} - val_acc: {logs.get('val_accuracy', 0):.4f}"
        self.training_tasks[self.task_id]['logs'].append(log_msg)


def load_training_data():
    """è¼‰å…¥è¨“ç·´è³‡æ–™"""
    X_data = []
    y_data = []
    
    # å®šç¾©é¡åˆ¥å°æ‡‰
    class_map = {'good': 0, 'normal': 1, 'bad': 2}
    
    # åˆå§‹åŒ–éª¨æ¶æå–å™¨
    pose_extractor = PoseExtractor()
    
    # æƒæå„å€‹é¡åˆ¥çš„å½±ç‰‡è³‡æ–™å¤¾
    for class_name, class_id in class_map.items():
        folder = f'{class_name}_input_movid'
        if not os.path.exists(folder):
            continue
        
        video_files = glob.glob(os.path.join(folder, '*.mp4')) + \
                      glob.glob(os.path.join(folder, '*.avi')) + \
                      glob.glob(os.path.join(folder, '*.MOV'))
        
        print(f"è™•ç† {class_name} é¡åˆ¥ï¼Œæ‰¾åˆ° {len(video_files)} å€‹å½±ç‰‡")
        
        for video_path in video_files:
            try:
                # æå–éª¨æ¶ç‰¹å¾µï¼ˆè¿”å›å­—å…¸åˆ—è¡¨ï¼‰
                pose_data = pose_extractor.extract_pose_data(video_path)
                
                if pose_data is None or len(pose_data) == 0:
                    print(f"è­¦å‘Šï¼š{video_path} æœªæå–åˆ°éª¨æ¶è³‡æ–™")
                    continue
                
                # å°‡éª¨æ¶è³‡æ–™è½‰æ›ç‚ºæ•¸å€¼é™£åˆ—
                landmarks_array = []
                for frame_data in pose_data:
                    if frame_data['landmarks'] is not None:
                        frame_landmarks = []
                        for lm in frame_data['landmarks']:
                            # åªä½¿ç”¨ x, y, z åº§æ¨™ï¼ˆå¿½ç•¥ visibilityï¼‰
                            frame_landmarks.extend([lm['x'], lm['y'], lm['z']])
                        landmarks_array.append(frame_landmarks)
                
                if len(landmarks_array) == 0:
                    print(f"è­¦å‘Šï¼š{video_path} æœªåµæ¸¬åˆ°æœ‰æ•ˆå¹€")
                    continue
                
                # è½‰æ›ç‚º numpy é™£åˆ—
                landmarks_array = np.array(landmarks_array)
                
                # æ¨™æº–åŒ–ç‚º 150 å¹€ Ã— 69 ç‰¹å¾µï¼ˆ23 å€‹é—œéµé» Ã— 3 åº§æ¨™ï¼‰
                # MediaPipe Pose æœ‰ 33 å€‹é—œéµé»ï¼Œæ’é™¤è‡‰éƒ¨å¾Œå‰© 23 å€‹
                target_frames = 150
                target_features = 69  # 23 é—œéµé» Ã— 3 åº§æ¨™ (x, y, z)
                
                # æª¢æŸ¥ç‰¹å¾µç¶­åº¦
                if landmarks_array.shape[1] != target_features:
                    print(f"è­¦å‘Šï¼š{video_path} ç‰¹å¾µç¶­åº¦ä¸ç¬¦ ({landmarks_array.shape[1]} != {target_features})ï¼Œè·³é")
                    continue
                
                # é‡æ–°æ¡æ¨£åˆ° 150 å¹€
                if landmarks_array.shape[0] != target_frames:
                    # ä½¿ç”¨ç·šæ€§æ’å€¼é‡æ–°æ¡æ¨£
                    original_frames = landmarks_array.shape[0]
                    resampled = np.zeros((target_frames, target_features))
                    
                    for feature_idx in range(target_features):
                        resampled[:, feature_idx] = np.interp(
                            np.linspace(0, original_frames - 1, target_frames),
                            np.arange(original_frames),
                            landmarks_array[:, feature_idx]
                        )
                    landmarks_array = resampled
                
                X_data.append(landmarks_array)
                y_data.append(class_id)
                
            except Exception as e:
                print(f"è™•ç†å½±ç‰‡ {video_path} æ™‚å‡ºéŒ¯: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    if len(X_data) == 0:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•è¨“ç·´è³‡æ–™ï¼Œè«‹ç¢ºèªå½±ç‰‡è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æœ‰æ•ˆå½±ç‰‡")
    
    print(f"æˆåŠŸè¼‰å…¥ {len(X_data)} å€‹è¨“ç·´æ¨£æœ¬")
    return np.array(X_data), np.array(y_data)


def create_model_basic(input_shape=(150, 69), num_classes=3):
    """åŸºç¤ LSTM æ¨¡å‹"""
    model = Sequential([
        LSTM(128, input_shape=input_shape, return_sequences=True),
        Dropout(0.4),
        LSTM(64),
        Dropout(0.4),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    return model


def create_model_bidirectional(input_shape=(150, 69), num_classes=3):
    """é›™å‘ LSTM æ¨¡å‹"""
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),
        Dropout(0.4),
        Bidirectional(LSTM(32)),
        Dropout(0.3),
        Dense(16, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    return model


def create_model_deep(input_shape=(150, 69), num_classes=3):
    """æ·±å±¤ LSTM æ¨¡å‹"""
    model = Sequential([
        LSTM(128, input_shape=input_shape, return_sequences=True),
        Dropout(0.4),
        LSTM(64, return_sequences=True),
        Dropout(0.4),
        LSTM(32),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    return model


def train_model(config, task_id, training_tasks):
    """
    åŸ·è¡Œæ¨¡å‹è¨“ç·´
    
    Args:
        config: è¨“ç·´é…ç½®
        task_id: ä»»å‹™ ID
        training_tasks: å…¨å±€ä»»å‹™å­—å…¸ï¼ˆç”¨æ–¼æ›´æ–°é€²åº¦ï¼‰
    
    Returns:
        è¨“ç·´çµæœå­—å…¸
    """
    try:
        start_time = datetime.now()
        
        # 1. è¼‰å…¥è³‡æ–™
        training_tasks[task_id]['message'] = 'æ­£åœ¨è¼‰å…¥è¨“ç·´è³‡æ–™...'
        training_tasks[task_id]['logs'].append('ğŸ“‚ è¼‰å…¥è¨“ç·´è³‡æ–™...')
        
        X_data, y_data = load_training_data()
        
        training_tasks[task_id]['logs'].append(f'âœ… è¼‰å…¥å®Œæˆï¼šå…± {len(X_data)} å€‹æ¨£æœ¬')
        training_tasks[task_id]['logs'].append(f'   - Good: {np.sum(y_data == 0)} å€‹')
        training_tasks[task_id]['logs'].append(f'   - Normal: {np.sum(y_data == 1)} å€‹')
        training_tasks[task_id]['logs'].append(f'   - Bad: {np.sum(y_data == 2)} å€‹')
        
        # æª¢æŸ¥æ¨£æœ¬æ•¸é‡æ˜¯å¦è¶³å¤ 
        min_samples_per_class = 10
        for class_id in [0, 1, 2]:
            class_count = np.sum(y_data == class_id)
            if class_count < min_samples_per_class:
                class_name = ['Good', 'Normal', 'Bad'][class_id]
                raise ValueError(
                    f'{class_name} é¡åˆ¥åªæœ‰ {class_count} å€‹æ¨£æœ¬ï¼Œè‡³å°‘éœ€è¦ {min_samples_per_class} å€‹ã€‚\n'
                    f'è«‹åœ¨å°æ‡‰è³‡æ–™å¤¾ä¸­æ·»åŠ æ›´å¤šå½±ç‰‡ï¼š\n'
                    f'  - Good â†’ backend/good_input_movid/\n'
                    f'  - Normal â†’ backend/normal_input_movid/\n'
                    f'  - Bad â†’ backend/bad_input_movid/'
                )
        
        if len(X_data) < 30:
            raise ValueError(
                f'ç¸½æ¨£æœ¬æ•¸åªæœ‰ {len(X_data)} å€‹ï¼Œè‡³å°‘éœ€è¦ 30 å€‹ï¼ˆæ¯é¡ 10 å€‹ï¼‰æ‰èƒ½é€²è¡Œè¨“ç·´ã€‚\n'
                f'ç•¶å‰ç‹€æ…‹ï¼š\n'
                f'  - Good: {np.sum(y_data == 0)} å€‹\n'
                f'  - Normal: {np.sum(y_data == 1)} å€‹\n'
                f'  - Bad: {np.sum(y_data == 2)} å€‹\n\n'
                f'å»ºè­°ï¼šæ¯å€‹é¡åˆ¥è‡³å°‘æº–å‚™ 30 å€‹å½±ç‰‡ï¼ˆç¸½å…± 90 å€‹ï¼‰ä»¥ç²å¾—è¼ƒå¥½çš„è¨“ç·´æ•ˆæœã€‚'
            )
        
        # 2. è³‡æ–™åˆ†å‰²
        training_tasks[task_id]['message'] = 'æ­£åœ¨åˆ†å‰²è³‡æ–™é›†...'
        
        # æ ¹æ“šæ¨£æœ¬æ•¸é‡å‹•æ…‹èª¿æ•´æ¸¬è©¦é›†æ¯”ä¾‹
        if len(X_data) < 50:
            test_size = 0.25  # å°æ•¸æ“šé›†ç”¨ 25%
            training_tasks[task_id]['logs'].append('âš ï¸  æ¨£æœ¬æ•¸è¼ƒå°‘ï¼Œä½¿ç”¨ 25% ä½œç‚ºæ¸¬è©¦é›†')
        else:
            test_size = 0.2   # æ­£å¸¸æƒ…æ³ç”¨ 20%
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_data, y_data, test_size=test_size, stratify=y_data, random_state=42
        )
        
        training_tasks[task_id]['logs'].append(f'âœ… è¨“ç·´é›†: {len(X_train)} æ¨£æœ¬ï¼Œæ¸¬è©¦é›†: {len(X_test)} æ¨£æœ¬')
        
        # 3. æ¨™æº–åŒ–
        training_tasks[task_id]['message'] = 'æ­£åœ¨æ¨™æº–åŒ–ç‰¹å¾µ...'
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train.reshape(-1, 69)).reshape(-1, 150, 69)
        X_test = scaler.transform(X_test.reshape(-1, 69)).reshape(-1, 150, 69)
        
        # å„²å­˜ scaler
        joblib.dump(scaler, 'scaler.pkl')
        training_tasks[task_id]['logs'].append('âœ… ç‰¹å¾µæ¨™æº–åŒ–å®Œæˆ')
        
        # 4. One-hot encoding
        from tensorflow.keras.utils import to_categorical
        y_train_cat = to_categorical(y_train, num_classes=3)
        y_test_cat = to_categorical(y_test, num_classes=3)
        
        # 5. å»ºç«‹æ¨¡å‹
        training_tasks[task_id]['message'] = f'æ­£åœ¨å»ºç«‹ {config["model_type"]} æ¨¡å‹...'
        
        if config['model_type'] == 'basic':
            model = create_model_basic()
        elif config['model_type'] == 'bidirectional':
            model = create_model_bidirectional()
        elif config['model_type'] == 'deep':
            model = create_model_deep()
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹é¡å‹: {config['model_type']}")
        
        model.compile(
            optimizer=Adam(learning_rate=config['learning_rate']),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        training_tasks[task_id]['logs'].append(f'âœ… {config["model_type"]} æ¨¡å‹å»ºç«‹å®Œæˆ')
        total_params = model.count_params()
        training_tasks[task_id]['logs'].append(f'   æ¨¡å‹åƒæ•¸é‡: {total_params:,}')
        
        # 6. è¨“ç·´æ¨¡å‹
        training_tasks[task_id]['message'] = 'é–‹å§‹è¨“ç·´æ¨¡å‹...'
        training_tasks[task_id]['logs'].append('ğŸ‹ï¸ é–‹å§‹è¨“ç·´...')
        
        progress_callback = TrainingProgressCallback(task_id, training_tasks, config['epochs'])
        
        history = model.fit(
            X_train, y_train_cat,
            validation_data=(X_test, y_test_cat),
            epochs=config['epochs'],
            batch_size=config['batch_size'],
            callbacks=[progress_callback],
            verbose=0  # ä¸åœ¨æ§åˆ¶å°è¼¸å‡ºï¼Œæ”¹ç”¨å›èª¿å‡½æ•¸
        )
        
        # 7. è©•ä¼°æ¨¡å‹
        training_tasks[task_id]['message'] = 'æ­£åœ¨è©•ä¼°æ¨¡å‹...'
        test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)
        
        training_tasks[task_id]['logs'].append(f'âœ… æ¸¬è©¦é›†æº–ç¢ºç‡: {test_acc:.4f}')
        training_tasks[task_id]['logs'].append(f'âœ… æ¸¬è©¦é›†æå¤±: {test_loss:.4f}')
        
        # 8. å„²å­˜æ¨¡å‹
        model_path = 'pose_classifier_model.h5'
        model.save(model_path)
        training_tasks[task_id]['logs'].append(f'âœ… æ¨¡å‹å·²å„²å­˜è‡³: {model_path}')
        
        # è¨ˆç®—è¨“ç·´æ™‚é–“
        end_time = datetime.now()
        training_time = str(end_time - start_time).split('.')[0]  # ç§»é™¤å¾®ç§’
        
        training_tasks[task_id]['logs'].append(f'ğŸ‰ è¨“ç·´å®Œæˆï¼ç¸½è€—æ™‚: {training_time}')
        
        # è¿”å›çµæœ
        return {
            'test_accuracy': float(test_acc),
            'test_loss': float(test_loss),
            'training_time': training_time,
            'model_path': model_path,
            'total_samples': len(X_data),
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'model_params': int(total_params)
        }
        
    except Exception as e:
        raise Exception(f'è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}')
