"""
å„ªåŒ–ç‰ˆè¨“ç·´è…³æœ¬ - æ•´åˆé€²éšæŠ€è¡“
åŸºæ–¼ MODEL_TRAINING_GUIDE.md çš„å»ºè­°å¯¦ä½œ

ä½¿ç”¨æ–¹å¼:
    python train_optimized.py --data_path ./training_data --epochs 150
"""

import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import (
    EarlyStopping, 
    ModelCheckpoint, 
    ReduceLROnPlateau,
    TensorBoard
)
from tensorflow.keras.optimizers import Adam
import os
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# ============ è³‡æ–™å¢å¼·å‡½æ•¸ ============

def temporal_scaling(sequence, scale_factor=0.2):
    """æ™‚é–“è»¸ç¸®æ”¾ï¼šéš¨æ©ŸåŠ é€Ÿæˆ–æ¸›é€Ÿå‹•ä½œ"""
    scale = np.random.uniform(1 - scale_factor, 1 + scale_factor)
    original_length = len(sequence)
    indices = np.linspace(0, original_length - 1, int(original_length * scale))
    
    scaled_seq = np.zeros((len(indices), sequence.shape[1]))
    for col in range(sequence.shape[1]):
        scaled_seq[:, col] = np.interp(indices, np.arange(original_length), sequence[:, col])
    
    # é‡æ–°æ¡æ¨£åˆ° 150 å¹€
    final_indices = np.linspace(0, len(scaled_seq) - 1, 150)
    final_seq = np.zeros((150, sequence.shape[1]))
    for col in range(sequence.shape[1]):
        final_seq[:, col] = np.interp(final_indices, np.arange(len(scaled_seq)), scaled_seq[:, col])
    
    return final_seq


def add_gaussian_noise(sequence, noise_level=0.01):
    """æ·»åŠ é«˜æ–¯å™ªè²æ¨¡æ“¬åµæ¸¬èª¤å·®"""
    noise = np.random.normal(0, noise_level, sequence.shape)
    return sequence + noise


def random_crop_pad(sequence, target_length=150, crop_ratio=0.1):
    """éš¨æ©Ÿè£å‰ªèˆ‡å¡«å……"""
    crop_length = int(len(sequence) * (1 - crop_ratio))
    start_idx = np.random.randint(0, max(1, len(sequence) - crop_length))
    cropped = sequence[start_idx:start_idx + crop_length]
    
    # å¡«å……å›ç›®æ¨™é•·åº¦
    indices = np.linspace(0, len(cropped) - 1, target_length)
    padded = np.zeros((target_length, sequence.shape[1]))
    for col in range(sequence.shape[1]):
        padded[:, col] = np.interp(indices, np.arange(len(cropped)), cropped[:, col])
    
    return padded


def augment_data(sequences, labels, augment_factor=3):
    """æ•´åˆå¢å¼·ç®¡ç·š"""
    aug_sequences = []
    aug_labels = []
    
    for seq, label in zip(sequences, labels):
        # ä¿ç•™åŸå§‹æ¨£æœ¬
        aug_sequences.append(seq)
        aug_labels.append(label)
        
        # ç”Ÿæˆå¢å¼·æ¨£æœ¬
        for _ in range(augment_factor):
            aug_seq = seq.copy()
            
            # éš¨æ©Ÿæ‡‰ç”¨å¢å¼·æŠ€è¡“
            if np.random.rand() > 0.5:
                aug_seq = temporal_scaling(aug_seq)
            if np.random.rand() > 0.5:
                aug_seq = add_gaussian_noise(aug_seq)
            if np.random.rand() > 0.3:
                aug_seq = random_crop_pad(aug_seq)
            
            aug_sequences.append(aug_seq)
            aug_labels.append(label)
    
    return np.array(aug_sequences), np.array(aug_labels)


# ============ æ¨¡å‹å»ºæ§‹å‡½æ•¸ ============

def create_model_basic(input_shape=(150, 69), num_classes=3):
    """åŸºç¤æ¨¡å‹ï¼ˆåŸå§‹æ¶æ§‹æ”¹è‰¯ç‰ˆï¼‰"""
    model = Sequential([
        LSTM(128, input_shape=input_shape, return_sequences=True),
        Dropout(0.4),
        LSTM(64),
        Dropout(0.4),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    return model


def create_model_bidirectional(input_shape=(150, 69), num_classes=3):
    """é›™å‘ LSTM æ¨¡å‹"""
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),
        Dropout(0.4),
        Bidirectional(LSTM(32)),
        Dropout(0.3),
        Dense(16, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    return model


def create_model_deep(input_shape=(150, 69), num_classes=3):
    """æ·±å±¤ LSTM æ¨¡å‹"""
    model = Sequential([
        LSTM(128, input_shape=input_shape, return_sequences=True),
        Dropout(0.4),
        LSTM(64, return_sequences=True),
        Dropout(0.4),
        LSTM(32),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    return model


# ============ è©•ä¼°èˆ‡è¦–è¦ºåŒ– ============

def plot_confusion_matrix(y_true, y_pred, class_names, save_path):
    """ç¹ªè£½æ··æ·†çŸ©é™£"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names,
        cbar_kws={'label': 'æ¨£æœ¬æ•¸é‡'}
    )
    plt.ylabel('å¯¦éš›é¡åˆ¥', fontsize=12)
    plt.xlabel('é æ¸¬é¡åˆ¥', fontsize=12)
    plt.title('æ··æ·†çŸ©é™£', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"âœ… æ··æ·†çŸ©é™£å·²å„²å­˜è‡³: {save_path}")


def plot_training_history(history, save_path):
    """ç¹ªè£½è¨“ç·´æ­·å²"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # æº–ç¢ºç‡
    axes[0].plot(history.history['accuracy'], label='è¨“ç·´æº–ç¢ºç‡', linewidth=2)
    axes[0].plot(history.history['val_accuracy'], label='é©—è­‰æº–ç¢ºç‡', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('æº–ç¢ºç‡', fontsize=12)
    axes[0].set_title('æ¨¡å‹æº–ç¢ºç‡', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # æå¤±
    axes[1].plot(history.history['loss'], label='è¨“ç·´æå¤±', linewidth=2)
    axes[1].plot(history.history['val_loss'], label='é©—è­‰æå¤±', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('æå¤±', fontsize=12)
    axes[1].set_title('æ¨¡å‹æå¤±', fontsize=14, fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"âœ… è¨“ç·´æ­·å²å·²å„²å­˜è‡³: {save_path}")


# ============ ä¸»è¨“ç·´æµç¨‹ ============

def main(args):
    print("=" * 60)
    print("ğŸš€ æ¡Œçƒå‹•ä½œåˆ†ææ¨¡å‹è¨“ç·´ - å„ªåŒ–ç‰ˆæœ¬")
    print("=" * 60)
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"training_output_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. è¼‰å…¥è³‡æ–™ï¼ˆéœ€è¦æ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´ï¼‰
    print("\nğŸ“‚ è¼‰å…¥è³‡æ–™...")
    # TODO: å¯¦ä½œä½ çš„è³‡æ–™è¼‰å…¥é‚è¼¯
    # X_data, y_data = load_your_data(args.data_path)
    
    # ç¤ºä¾‹ï¼šå‡è¨­å·²æœ‰è³‡æ–™
    # X_data shape: (num_samples, 150, 69)
    # y_data shape: (num_samples,) - é¡åˆ¥æ¨™ç±¤ 0/1/2
    
    # æš«æ™‚ä½¿ç”¨éš¨æ©Ÿè³‡æ–™ç¤ºç¯„
    print("âš ï¸  ä½¿ç”¨éš¨æ©Ÿè³‡æ–™ç¤ºç¯„ï¼ˆè«‹æ›¿æ›ç‚ºçœŸå¯¦è³‡æ–™ï¼‰")
    X_data = np.random.randn(300, 150, 69)
    y_data = np.random.randint(0, 3, 300)
    
    # 2. è³‡æ–™åˆ†å‰²
    print("\nâœ‚ï¸  åˆ†å‰²è³‡æ–™é›†...")
    X_temp, X_test, y_temp, y_test = train_test_split(
        X_data, y_data, test_size=0.15, stratify=y_data, random_state=42
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.176, stratify=y_temp, random_state=42  # 0.176 * 0.85 â‰ˆ 0.15
    )
    
    print(f"è¨“ç·´é›†: {X_train.shape[0]} æ¨£æœ¬")
    print(f"é©—è­‰é›†: {X_val.shape[0]} æ¨£æœ¬")
    print(f"æ¸¬è©¦é›†: {X_test.shape[0]} æ¨£æœ¬")
    
    # 3. è³‡æ–™å¢å¼·
    if args.use_augmentation:
        print(f"\nğŸ”„ æ‡‰ç”¨è³‡æ–™å¢å¼· (æ“´å¢å› å­={args.augment_factor})...")
        X_train_aug, y_train_aug = augment_data(X_train, y_train, augment_factor=args.augment_factor)
        print(f"å¢å¼·å¾Œè¨“ç·´é›†: {X_train_aug.shape[0]} æ¨£æœ¬")
    else:
        X_train_aug, y_train_aug = X_train, y_train
    
    # 4. æ¨™æº–åŒ–
    print("\nğŸ“Š æ¨™æº–åŒ–ç‰¹å¾µ...")
    scaler = StandardScaler()
    X_train_aug = scaler.fit_transform(X_train_aug.reshape(-1, 69)).reshape(-1, 150, 69)
    X_val = scaler.transform(X_val.reshape(-1, 69)).reshape(-1, 150, 69)
    X_test = scaler.transform(X_test.reshape(-1, 69)).reshape(-1, 150, 69)
    
    # å„²å­˜ scaler
    import joblib
    scaler_path = os.path.join(output_dir, 'scaler.pkl')
    joblib.dump(scaler, scaler_path)
    print(f"âœ… Scaler å·²å„²å­˜è‡³: {scaler_path}")
    
    # 5. One-hot encoding
    from tensorflow.keras.utils import to_categorical
    y_train_cat = to_categorical(y_train_aug, num_classes=3)
    y_val_cat = to_categorical(y_val, num_classes=3)
    y_test_cat = to_categorical(y_test, num_classes=3)
    
    # 6. è¨ˆç®—é¡åˆ¥æ¬Šé‡
    print("\nâš–ï¸  è¨ˆç®—é¡åˆ¥æ¬Šé‡...")
    class_weights = compute_class_weight(
        'balanced',
        classes=np.unique(y_train_aug),
        y=y_train_aug
    )
    class_weight_dict = dict(enumerate(class_weights))
    print(f"é¡åˆ¥æ¬Šé‡: {class_weight_dict}")
    
    # 7. å»ºç«‹æ¨¡å‹
    print(f"\nğŸ§  å»ºç«‹æ¨¡å‹ (æ¶æ§‹={args.model_type})...")
    if args.model_type == 'basic':
        model = create_model_basic()
    elif args.model_type == 'bidirectional':
        model = create_model_bidirectional()
    elif args.model_type == 'deep':
        model = create_model_deep()
    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹é¡å‹: {args.model_type}")
    
    model.compile(
        optimizer=Adam(learning_rate=args.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    model.summary()
    
    # 8. é…ç½® Callbacks
    print("\nâš™ï¸  é…ç½®è¨“ç·´å›èª¿...")
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=args.early_stop_patience,
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            os.path.join(output_dir, 'best_model.h5'),
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            verbose=1
        ),
        TensorBoard(
            log_dir=os.path.join(output_dir, 'logs'),
            histogram_freq=1
        )
    ]
    
    # 9. è¨“ç·´æ¨¡å‹
    print("\nğŸ‹ï¸  é–‹å§‹è¨“ç·´...")
    history = model.fit(
        X_train_aug, y_train_cat,
        validation_data=(X_val, y_val_cat),
        epochs=args.epochs,
        batch_size=args.batch_size,
        class_weight=class_weight_dict,
        callbacks=callbacks,
        verbose=1
    )
    
    # 10. è©•ä¼°æ¨¡å‹
    print("\nğŸ“ˆ è©•ä¼°æ¨¡å‹...")
    test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)
    print(f"âœ… æ¸¬è©¦é›†æº–ç¢ºç‡: {test_acc:.4f}")
    print(f"âœ… æ¸¬è©¦é›†æå¤±: {test_loss:.4f}")
    
    # 11. é æ¸¬èˆ‡åˆ†æ
    print("\nğŸ” ç”Ÿæˆé æ¸¬...")
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test_cat, axis=1)
    
    # 12. æ··æ·†çŸ©é™£
    class_names = ['Bad', 'Good', 'Normal']
    cm_path = os.path.join(output_dir, 'confusion_matrix.png')
    plot_confusion_matrix(y_true_classes, y_pred_classes, class_names, cm_path)
    
    # 13. åˆ†é¡å ±å‘Š
    print("\nğŸ“Š åˆ†é¡å ±å‘Š:")
    report = classification_report(
        y_true_classes, 
        y_pred_classes,
        target_names=class_names,
        digits=4
    )
    print(report)
    
    # å„²å­˜å ±å‘Š
    report_path = os.path.join(output_dir, 'classification_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"âœ… åˆ†é¡å ±å‘Šå·²å„²å­˜è‡³: {report_path}")
    
    # 14. è¨“ç·´æ­·å²è¦–è¦ºåŒ–
    history_path = os.path.join(output_dir, 'training_history.png')
    plot_training_history(history, history_path)
    
    # 15. å„²å­˜æœ€çµ‚æ¨¡å‹
    final_model_path = os.path.join(output_dir, 'final_model.h5')
    model.save(final_model_path)
    print(f"\nâœ… æœ€çµ‚æ¨¡å‹å·²å„²å­˜è‡³: {final_model_path}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰è¼¸å‡ºå·²å„²å­˜è‡³: {output_dir}")
    print("=" * 60)
    
    # è¿”å›çµæœ
    return {
        'model': model,
        'history': history,
        'test_accuracy': test_acc,
        'output_dir': output_dir
    }


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å„ªåŒ–ç‰ˆæ¡Œçƒå‹•ä½œåˆ†ææ¨¡å‹è¨“ç·´')
    
    # è³‡æ–™åƒæ•¸
    parser.add_argument('--data_path', type=str, default='./training_data',
                        help='è¨“ç·´è³‡æ–™è·¯å¾‘')
    
    # æ¨¡å‹åƒæ•¸
    parser.add_argument('--model_type', type=str, default='basic',
                        choices=['basic', 'bidirectional', 'deep'],
                        help='æ¨¡å‹æ¶æ§‹é¡å‹')
    
    # è¨“ç·´åƒæ•¸
    parser.add_argument('--epochs', type=int, default=150,
                        help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                        help='å­¸ç¿’ç‡')
    parser.add_argument('--early_stop_patience', type=int, default=15,
                        help='æ—©åœè€å¿ƒå€¼')
    
    # è³‡æ–™å¢å¼·åƒæ•¸
    parser.add_argument('--use_augmentation', action='store_true',
                        help='æ˜¯å¦ä½¿ç”¨è³‡æ–™å¢å¼·')
    parser.add_argument('--augment_factor', type=int, default=3,
                        help='è³‡æ–™å¢å¼·å› å­')
    
    args = parser.parse_args()
    
    # åŸ·è¡Œè¨“ç·´
    results = main(args)
